{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff7956a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.13.7\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e172cd32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.1+cu128'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb5b37eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3050 6GB Laptop GPU'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64cdd564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset download link:  https://drive.google.com/uc?id=0B7EVK8r0v71pZjFTYXZWM3FlRnM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1a9d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4fd6dbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CelebADataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Get all image file paths from the directory\n",
    "        self.image_paths = [os.path.join(root_dir, img) for img in os.listdir(root_dir) if img.endswith('.jpg')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Apply the transform if provided\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8aaff3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:10: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:10: SyntaxWarning: invalid escape sequence '\\S'\n",
      "C:\\Users\\sarah\\AppData\\Local\\Temp\\ipykernel_5988\\2152129532.py:10: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  dataset_path = 'D:\\Studies\\GAN\\demo\\img_align_celeba\\img_align_celeba'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images loaded: 202599\n"
     ]
    }
   ],
   "source": [
    "# Define transformations (resize, crop, convert to tensor, normalize)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(64),  # Resize images to 64x64\n",
    "    transforms.CenterCrop(64),  # Crop center to 64x64\n",
    "    transforms.ToTensor(),  # Convert images to tensor\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Load CelebA dataset from the specified directory\n",
    "dataset_path = 'D:\\Studies\\GAN\\demo\\img_align_celeba\\img_align_celeba'  \n",
    "dataset = CelebADataset(root_dir=dataset_path, transform=transform)\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Check how many images are loaded\n",
    "print(f\"Total number of images loaded: {len(dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c012cf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator and Discriminator classes (same as previously described)\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim=100, img_channels=3):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(z_dim, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1024, img_channels * 64 * 64),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.size(0), 3, 64, 64)  # Reshape to image format\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ba58d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(img_channels * 64 * 64, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "750ca894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizers\n",
    "adversarial_loss = nn.BCELoss()\n",
    "generator = Generator(z_dim=100)\n",
    "discriminator = Discriminator()\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# Define device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "generator = generator.to(device)\n",
    "discriminator = discriminator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63522cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/5] [Batch 0/1583] [D loss: 0.6923354268074036] [G loss: 0.718485414981842]\n",
      "[Epoch 0/5] [Batch 50/1583] [D loss: 0.29669204354286194] [G loss: 0.8617404103279114]\n",
      "[Epoch 0/5] [Batch 100/1583] [D loss: 0.1929788589477539] [G loss: 1.8163785934448242]\n",
      "[Epoch 0/5] [Batch 150/1583] [D loss: 0.14327573776245117] [G loss: 1.6827995777130127]\n",
      "[Epoch 0/5] [Batch 200/1583] [D loss: 0.061873115599155426] [G loss: 2.182161569595337]\n",
      "[Epoch 0/5] [Batch 250/1583] [D loss: 0.09549085795879364] [G loss: 3.3741583824157715]\n",
      "[Epoch 0/5] [Batch 300/1583] [D loss: 0.12486374378204346] [G loss: 2.6637368202209473]\n",
      "[Epoch 0/5] [Batch 350/1583] [D loss: 0.16497530043125153] [G loss: 3.03373384475708]\n",
      "[Epoch 0/5] [Batch 400/1583] [D loss: 0.12775090336799622] [G loss: 6.45664119720459]\n",
      "[Epoch 0/5] [Batch 450/1583] [D loss: 0.0343310609459877] [G loss: 3.842803478240967]\n",
      "[Epoch 0/5] [Batch 500/1583] [D loss: 0.04248695820569992] [G loss: 2.9072413444519043]\n",
      "[Epoch 0/5] [Batch 550/1583] [D loss: 0.04489893466234207] [G loss: 3.579031229019165]\n",
      "[Epoch 0/5] [Batch 600/1583] [D loss: 0.23525698482990265] [G loss: 3.734426975250244]\n",
      "[Epoch 0/5] [Batch 650/1583] [D loss: 0.06533689051866531] [G loss: 5.251291275024414]\n",
      "[Epoch 0/5] [Batch 700/1583] [D loss: 0.044882532209157944] [G loss: 6.82928466796875]\n",
      "[Epoch 0/5] [Batch 750/1583] [D loss: 0.4976038932800293] [G loss: 7.269022464752197]\n",
      "[Epoch 0/5] [Batch 800/1583] [D loss: 0.06567570567131042] [G loss: 4.640132904052734]\n",
      "[Epoch 0/5] [Batch 850/1583] [D loss: 0.036904461681842804] [G loss: 6.04987096786499]\n",
      "[Epoch 0/5] [Batch 900/1583] [D loss: 0.03506507724523544] [G loss: 4.497796058654785]\n",
      "[Epoch 0/5] [Batch 950/1583] [D loss: 0.6045132875442505] [G loss: 9.23686408996582]\n",
      "[Epoch 0/5] [Batch 1000/1583] [D loss: 0.020915793254971504] [G loss: 4.751725196838379]\n",
      "[Epoch 0/5] [Batch 1050/1583] [D loss: 0.10324516892433167] [G loss: 6.480940818786621]\n",
      "[Epoch 0/5] [Batch 1100/1583] [D loss: 0.07750857621431351] [G loss: 4.831910133361816]\n",
      "[Epoch 0/5] [Batch 1150/1583] [D loss: 0.17477121949195862] [G loss: 7.887207984924316]\n",
      "[Epoch 0/5] [Batch 1200/1583] [D loss: 0.04145926982164383] [G loss: 5.579791069030762]\n",
      "[Epoch 0/5] [Batch 1250/1583] [D loss: 0.050947003066539764] [G loss: 3.869929790496826]\n",
      "[Epoch 0/5] [Batch 1300/1583] [D loss: 0.16787047684192657] [G loss: 5.360347747802734]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     38\u001b[39m     plt.show()\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(generator, discriminator, dataloader, epochs)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain\u001b[39m(generator, discriminator, dataloader, epochs=\u001b[32m5\u001b[39m):\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreal_imgs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_imgs\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\utils\\data\\dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\utils\\data\\dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mCelebADataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[32m     18\u001b[39m     \u001b[38;5;66;03m# Load image\u001b[39;00m\n\u001b[32m     19\u001b[39m     img_path = \u001b[38;5;28mself\u001b[39m.image_paths[idx]\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     image = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m.convert(\u001b[33m'\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     22\u001b[39m     \u001b[38;5;66;03m# Apply the transform if provided\u001b[39;00m\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\PIL\\Image.py:3513\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, formats)\u001b[39m\n\u001b[32m   3511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_path(fp):\n\u001b[32m   3512\u001b[39m     filename = os.fspath(fp)\n\u001b[32m-> \u001b[39m\u001b[32m3513\u001b[39m     fp = \u001b[43mbuiltins\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   3514\u001b[39m     exclusive_fp = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   3515\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from torchvision.utils import save_image\n",
    "# Training loop\n",
    "def train(generator, discriminator, dataloader, epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        for i, imgs in enumerate(dataloader):\n",
    "            real_imgs = imgs.to(device)\n",
    "            batch_size = real_imgs.size(0)\n",
    "            valid = torch.ones(batch_size, 1).to(device)\n",
    "            fake = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "            # Train Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "            real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
    "            fake_loss = adversarial_loss(discriminator(generator(torch.randn(batch_size, 100).to(device)).detach()), fake)\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # Train Generator\n",
    "            optimizer_G.zero_grad()\n",
    "            g_loss = adversarial_loss(discriminator(generator(torch.randn(batch_size, 100).to(device))), valid)\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            if i % 50 == 0:\n",
    "                print(f\"[Epoch {epoch}/{epochs}] [Batch {i}/{len(dataloader)}] [D loss: {d_loss.item()}] [G loss: {g_loss.item()}]\")\n",
    "\n",
    "        # Optionally, save generated images at each epoch\n",
    "        save_generated_images(generator, epoch, device)\n",
    "\n",
    "def save_generated_images(generator, epoch, device, num_images=16):\n",
    "    z = torch.randn(num_images, 100).to(device)\n",
    "    generated_imgs = generator(z).detach().cpu()\n",
    "    grid = torchvision.utils.make_grid(generated_imgs, nrow=4, normalize=True)\n",
    "    plt.imshow(np.transpose(grid, (1, 2, 0)))\n",
    "    plt.title(f\"Epoch {epoch}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Start training\n",
    "train(generator, discriminator, dataloader, epochs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
